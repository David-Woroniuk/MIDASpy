{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDAS demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a brief demonstration of the **midas** class in the Python programming environment, the software we have developed to implement MIDAS. We show how to use the class to multiply impute missing values in the Adult census dataset, the basis for our applied accuracy test.\n",
    "\n",
    "To access the class, users must have TensorFlow installed as a **pip** package in their Python environment. We recommend creating a Conda environment before reinstalling TensorFlow to avoid conflicts with projects using later versions of the package. MIDAS is written in TensorFlow 1.X API; users of TensorFlow 2.X can install the correct version via the command line `pip install tensorflow==1.14.0`\n",
    "\n",
    "\n",
    "Once these packages have been installed, users can import the dependencies and load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'age', 'workclass', 'fnlwgt', 'education',\n",
       "       'education_num', 'marital_status', 'occupation', 'relationship', 'race',\n",
       "       'sex', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
       "       'native_country', 'class_labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from midas import Midas\n",
    "\n",
    "data_0 = pd.read_csv('data/adult_data.csv')\n",
    "data_0.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset has a very low proportion of missingness (one of the reasons we selected it for the accuracy test), we randomly set 5000 observed values as missing in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(441)\n",
    "\n",
    "def spike_in_generation(data):\n",
    "    spike_in = pd.DataFrame(np.zeros_like(data), columns= data.columns)\n",
    "    for column in data.columns:\n",
    "        subset = np.random.choice(data[column].index[data[column].notnull()], 5000, replace= False)\n",
    "        spike_in.loc[subset, column] = 1\n",
    "    return spike_in\n",
    "\n",
    "spike_in = spike_in_generation(data_0)\n",
    "original_value = data_0.loc[4, 'hours_per_week']\n",
    "data_0[spike_in == 1] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we list categorical variables in a vector and one-hot encode them using an inbuilt function in the pandas package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['workclass','marital_status','relationship','race','class_labels','sex','education','occupation','native_country']\n",
    "\n",
    "data_1 = data_0[categorical]\n",
    "data_0.drop(categorical, axis = 1, inplace = True)\n",
    "\n",
    "constructor_list = [data_0]\n",
    "columns_list = []\n",
    "\n",
    "for column in data_1.columns:\n",
    "    na_temp = data_1[column].isnull()\n",
    "    temp = pd.get_dummies(data_1[column], prefix = column)\n",
    "    temp[na_temp] = np.nan\n",
    "    constructor_list.append(temp)\n",
    "    columns_list.append(list(temp.columns.values))\n",
    "    \n",
    "data_0 = pd.concat(constructor_list, axis=1)\n",
    "\n",
    "na_loc = data_0.isnull()\n",
    "data_0[na_loc] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   age    fnlwgt  education_num  capital_gain  capital_loss  \\\n",
      "0         0.0  39.0   77516.0           13.0        2174.0           0.0   \n",
      "1         1.0  50.0   83311.0           13.0           0.0           0.0   \n",
      "2         2.0  38.0  215646.0            9.0           0.0           0.0   \n",
      "3         3.0  53.0  234721.0            NaN           0.0           0.0   \n",
      "4         4.0  28.0       NaN           13.0           0.0           NaN   \n",
      "\n",
      "   hours_per_week  workclass_Federal-gov  workclass_Local-gov  \\\n",
      "0            40.0                    0.0                  0.0   \n",
      "1            13.0                    0.0                  0.0   \n",
      "2            40.0                    0.0                  0.0   \n",
      "3            40.0                    0.0                  0.0   \n",
      "4             NaN                    0.0                  0.0   \n",
      "\n",
      "   workclass_Never-worked  ...  native_country_Portugal  \\\n",
      "0                     0.0  ...                      0.0   \n",
      "1                     0.0  ...                      0.0   \n",
      "2                     0.0  ...                      0.0   \n",
      "3                     0.0  ...                      0.0   \n",
      "4                     0.0  ...                      0.0   \n",
      "\n",
      "   native_country_Puerto-Rico  native_country_Scotland  native_country_South  \\\n",
      "0                         0.0                      0.0                   0.0   \n",
      "1                         0.0                      0.0                   0.0   \n",
      "2                         0.0                      0.0                   0.0   \n",
      "3                         0.0                      0.0                   0.0   \n",
      "4                         0.0                      0.0                   0.0   \n",
      "\n",
      "   native_country_Taiwan  native_country_Thailand  \\\n",
      "0                    0.0                      0.0   \n",
      "1                    0.0                      0.0   \n",
      "2                    0.0                      0.0   \n",
      "3                    0.0                      0.0   \n",
      "4                    0.0                      0.0   \n",
      "\n",
      "   native_country_Trinadad&Tobago  native_country_United-States  \\\n",
      "0                             0.0                           1.0   \n",
      "1                             0.0                           1.0   \n",
      "2                             0.0                           1.0   \n",
      "3                             0.0                           1.0   \n",
      "4                             0.0                           0.0   \n",
      "\n",
      "   native_country_Vietnam  native_country_Yugoslavia  \n",
      "0                     0.0                        0.0  \n",
      "1                     0.0                        0.0  \n",
      "2                     0.0                        0.0  \n",
      "3                     0.0                        0.0  \n",
      "4                     0.0                        0.0  \n",
      "\n",
      "[5 rows x 108 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_0.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are now ready to be fed into the midas algorithm, which involves three steps. First, we specify the dimensions, input corruption proportion, and other hyperparameters of the MIDAS neural network. Second, we build a MIDAS model based on the data. The vector of one-hot-encoded column names should be passed to the softmax_columns argument, as MIDAS employs a softmax final-layer activation function for categorical variables. Third, we train the model on the data, setting the number of training epochs as 20 in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [7, 8, 7, 6, 5, 2, 2, 16, 14, 41]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 133848.50805376086\n",
      "Epoch: 1 , loss: 95065.40653797715\n",
      "Epoch: 2 , loss: 90628.25024318071\n",
      "Epoch: 3 , loss: 85635.75156979542\n",
      "Epoch: 4 , loss: 79943.5477344518\n",
      "Epoch: 5 , loss: 76176.41991035591\n",
      "Epoch: 6 , loss: 75168.03345590494\n",
      "Epoch: 7 , loss: 73609.58660368713\n",
      "Epoch: 8 , loss: 73962.98218317394\n",
      "Epoch: 9 , loss: 73652.5491948159\n",
      "Epoch: 10 , loss: 72959.83611860563\n",
      "Epoch: 11 , loss: 73128.3418826282\n",
      "Epoch: 12 , loss: 73564.0481312203\n",
      "Epoch: 13 , loss: 73355.49027725159\n",
      "Epoch: 14 , loss: 72929.31829857982\n",
      "Epoch: 15 , loss: 72174.56946968689\n",
      "Epoch: 16 , loss: 73270.2137865539\n",
      "Epoch: 17 , loss: 71626.98738724095\n",
      "Epoch: 18 , loss: 72509.55602243406\n",
      "Epoch: 19 , loss: 72089.0846345634\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<midas.midas_base.Midas at 0x1a32e33438>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = Midas(layer_structure = [256,256], vae_layer = False, seed = 89, input_drop = 0.75)\n",
    "imputer.build_model(data_0, softmax_columns = columns_list)\n",
    "imputer.train_model(training_epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, we can generate any number of imputed datasets using the generate_samples function (here we set M as 10). Users can then either write these imputations to separate .csv files or work with them directly in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "imputations = imputer.generate_samples(m=10).output_list \n",
    "\n",
    "# for i in imputations:\n",
    "#    file_out = ``midas_imp_\" + str(n) + ``.csv\"\n",
    "#    i.to_csv(file_out, index=False)\n",
    "#    n += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
