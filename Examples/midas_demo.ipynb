{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MIDAS demonstration"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides a brief demonstration of **MIDASpy**'s core functionalities. We show how to use the package to impute missing values in the Adult census dataset (which is commonly used for benchmarking in the machine learning literature).\n",
        "\n",
        "Users of **MIDASpy** must have TensorFlow installed as a **pip** package in their Python environment. MIDAS is compatible with both TensorFlow 1.X and TensorFlow >= 2.2 versions.\n",
        "\n\nOnce these packages have been installed, users can import the dependencies and load the data:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import MIDASpy as md\n",
        "\n",
        "data_0 = pd.read_csv('adult_data.csv')\n",
        "data_0.columns.str.strip()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'age', 'workclass', 'fnlwgt', 'education',\n",
              "       'education_num', 'marital_status', 'occupation', 'relationship', 'race',\n",
              "       'sex', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
              "       'native_country', 'class_labels'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the dataset has a very low proportion of missingness (one of the reasons we selected it for the accuracy test), we randomly set 5000 observed values as missing in each column:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(441)\n",
        "\n",
        "def spike_in_generation(data):\n",
        "    spike_in = pd.DataFrame(np.zeros_like(data), columns= data.columns)\n",
        "    for column in data.columns:\n",
        "        subset = np.random.choice(data[column].index[data[column].notnull()], 5000, replace= False)\n",
        "        spike_in.loc[subset, column] = 1\n",
        "    return spike_in\n",
        "\n",
        "spike_in = spike_in_generation(data_0)\n",
        "original_value = data_0.loc[4, 'hours_per_week']\n",
        "data_0[spike_in == 1] = np.nan"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we list categorical variables in a vector and one-hot encode them using **MIDASpy**'s inbuilt preprocessing function `cat_conv`, which returns both the encoded data and a nested list of categorical column names we can pass to the imputation algorithm. To construct the final, pre-processed data we append the one-hot encoded categorical data to the non-cateogrical data, and replace null values with `np.nan` values:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "categorical = ['workclass','marital_status','relationship','race','class_labels','sex','education','occupation','native_country']\n",
        "data_cat, cat_cols_list = md.cat_conv(data_0[categorical])\n",
        "\n",
        "data_0.drop(categorical, axis = 1, inplace = True)\n",
        "constructor_list = [data_0]\n",
        "constructor_list.append(data_cat)\n",
        "data_in = pd.concat(constructor_list, axis=1)\n",
        "\n",
        "na_loc = data_in.isnull()\n",
        "data_in[na_loc] = np.nan"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualise the results:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_in.head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0   age    fnlwgt  education_num  capital_gain  capital_loss  \\\n",
            "0         0.0  39.0   77516.0           13.0        2174.0           0.0   \n",
            "1         1.0  50.0   83311.0           13.0           0.0           0.0   \n",
            "2         2.0  38.0  215646.0            9.0           0.0           0.0   \n",
            "3         3.0  53.0  234721.0            NaN           0.0           0.0   \n",
            "4         4.0  28.0       NaN           13.0           0.0           NaN   \n",
            "\n",
            "   hours_per_week  workclass_Federal-gov  workclass_Local-gov  \\\n",
            "0            40.0                    0.0                  0.0   \n",
            "1            13.0                    0.0                  0.0   \n",
            "2            40.0                    0.0                  0.0   \n",
            "3            40.0                    0.0                  0.0   \n",
            "4             NaN                    0.0                  0.0   \n",
            "\n",
            "   workclass_Never-worked  ...  native_country_Portugal  \\\n",
            "0                     0.0  ...                      0.0   \n",
            "1                     0.0  ...                      0.0   \n",
            "2                     0.0  ...                      0.0   \n",
            "3                     0.0  ...                      0.0   \n",
            "4                     0.0  ...                      0.0   \n",
            "\n",
            "   native_country_Puerto-Rico  native_country_Scotland  native_country_South  \\\n",
            "0                         0.0                      0.0                   0.0   \n",
            "1                         0.0                      0.0                   0.0   \n",
            "2                         0.0                      0.0                   0.0   \n",
            "3                         0.0                      0.0                   0.0   \n",
            "4                         0.0                      0.0                   0.0   \n",
            "\n",
            "   native_country_Taiwan  native_country_Thailand  \\\n",
            "0                    0.0                      0.0   \n",
            "1                    0.0                      0.0   \n",
            "2                    0.0                      0.0   \n",
            "3                    0.0                      0.0   \n",
            "4                    0.0                      0.0   \n",
            "\n",
            "   native_country_Trinadad&Tobago  native_country_United-States  \\\n",
            "0                             0.0                           1.0   \n",
            "1                             0.0                           1.0   \n",
            "2                             0.0                           1.0   \n",
            "3                             0.0                           1.0   \n",
            "4                             0.0                           0.0   \n",
            "\n",
            "   native_country_Vietnam  native_country_Yugoslavia  \n",
            "0                     0.0                        0.0  \n",
            "1                     0.0                        0.0  \n",
            "2                     0.0                        0.0  \n",
            "3                     0.0                        0.0  \n",
            "4                     0.0                        0.0  \n",
            "\n",
            "[5 rows x 108 columns]\n"
          ]
        }
      ],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data are now ready to be fed into the MIDAS algorithm, which involves three steps. First, we specify the dimensions, input corruption proportion, and other hyperparameters of the MIDAS neural network. Second, we build a MIDAS model based on the data. The vector of one-hot-encoded column names should be passed to the softmax_columns argument, as MIDAS employs a softmax final-layer activation function for categorical variables. Third, we train the model on the data, setting the number of training epochs as 20 in this example:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = md.Midas(layer_structure = [256,256], vae_layer = False, seed = 89, input_drop = 0.75)\n",
        "imputer.build_model(data_in, softmax_columns = cat_cols_list)\n",
        "imputer.train_model(training_epochs = 20)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size index: [7, 8, 7, 6, 5, 2, 2, 16, 14, 41]\n",
            "\n",
            "Computation graph constructed\n",
            "\n",
            "Model initialised\n",
            "\n",
            "Epoch: 0 , loss: 131061.55897771953\n",
            "Epoch: 1 , loss: 94879.03236991112\n",
            "Epoch: 2 , loss: 90927.99433900925\n",
            "Epoch: 3 , loss: 88644.25104503706\n",
            "Epoch: 4 , loss: 85632.2174963395\n",
            "Epoch: 5 , loss: 80600.93999836173\n",
            "Epoch: 6 , loss: 76585.74140164237\n",
            "Epoch: 7 , loss: 75515.47067752703\n",
            "Epoch: 8 , loss: 74397.19318722867\n",
            "Epoch: 9 , loss: 73943.73610222293\n",
            "Epoch: 10 , loss: 74009.57563943726\n",
            "Epoch: 11 , loss: 74607.9188243621\n",
            "Epoch: 12 , loss: 73648.4787952828\n",
            "Epoch: 13 , loss: 73824.28025167923\n",
            "Epoch: 14 , loss: 73013.18742640584\n",
            "Epoch: 15 , loss: 73926.46652169684\n",
            "Epoch: 16 , loss: 72989.6765536687\n",
            "Epoch: 17 , loss: 73968.84397654202\n",
            "Epoch: 18 , loss: 73285.7860169817\n",
            "Epoch: 19 , loss: 73451.92790332159\n",
            "Training complete. Saving file...\n",
            "Model saved in file: tmp/MIDAS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": [
              "<MIDASpy.midas_base.Midas at 0x7fc5292e25e0>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once training is complete, we can generate any number of imputed datasets using the generate_samples function (here we set M as 10). Users can then either write these imputations to separate .csv files or work with them directly in Python:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "imputations = imputer.generate_samples(m=10).output_list \n",
        "\n",
        "# for i in imputations:\n",
        "#    file_out = ``midas_imp_\" + str(n) + ``.csv\"\n",
        "#    i.to_csv(file_out, index=False)\n",
        "#    n += 1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
            "Model restored.\n"
          ]
        }
      ],
      "execution_count": 30,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
